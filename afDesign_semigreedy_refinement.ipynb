{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "afDesign_semigreedy_refinement.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1KHZ_t97B9Y_RhuLW0FEQv3CcdgmShH8E",
      "authorship_tag": "ABX9TyOWveGwT0EdgBdG5MPOHRzz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sokrypton/AccAdam_TF2/blob/main/afDesign_semigreedy_refinement.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IOr3jQEvoe6",
        "outputId": "89f18db0-1d31-4a15-9dfb-852323c97349"
      },
      "source": [
        "%%bash\n",
        "if [ ! -d afDesign ]; then\n",
        "  git clone https://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@github.com/sokrypton/afDesign.git\n",
        "  pip -q install py3Dmol biopython dm-haiku ml_collections\n",
        "fi\n",
        "if [ ! -d params ]; then\n",
        "  mkdir params\n",
        "  curl -fsSL https://storage.googleapis.com/alphafold/alphafold_params_2021-07-14.tar | tar x -C params\n",
        "fi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'afDesign'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ym3Vie7v1Yb"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.append('afDesign')\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import py3Dmol\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "from jax.experimental.optimizers import adam\n",
        "\n",
        "from alphafold.common import protein\n",
        "from alphafold.data import pipeline\n",
        "from alphafold.model import data, config, model, modules\n",
        "from alphafold.common import residue_constants\n",
        "\n",
        "from alphafold.model import all_atom\n",
        "from alphafold.model import folding\n",
        "\n",
        "# custom functions\n",
        "from alphafold.data import prep_inputs\n",
        "from utils import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shh_V1eswjrH"
      },
      "source": [
        "# setup which model params to use\n",
        "model_name = \"model_3_ptm\"\n",
        "model_config = config.model_config(model_name)\n",
        "\n",
        "# enable checkpointing\n",
        "model_config.model.global_config.use_remat = True\n",
        "\n",
        "# number of recycles\n",
        "model_config.model.num_recycle = 3\n",
        "model_config.data.common.num_recycle = 3\n",
        "\n",
        "# backprop through recycles\n",
        "model_config.model.backprop_recycle = False\n",
        "model_config.model.embeddings_and_evoformer.backprop_dgram = False\n",
        "\n",
        "# number of sequences\n",
        "N = 1\n",
        "model_config.data.eval.max_msa_clusters = N\n",
        "model_config.data.common.max_extra_msa = 1\n",
        "model_config.data.eval.masked_msa_replace_fraction = 0\n",
        "\n",
        "# dropout\n",
        "model_config = set_dropout(model_config, 0.0)\n",
        "\n",
        "# setup model\n",
        "model_params = [data.get_model_haiku_params(model_name=model_name, data_dir=\".\")]\n",
        "model_runner = model.RunModel(model_config, model_params[0], is_training=True)\n",
        "\n",
        "# load the other models to sample during design.\n",
        "for model_name in [\"model_1_ptm\",\"model_2_ptm\",\"model_4_ptm\",\"model_5_ptm\"]:\n",
        "  params = data.get_model_haiku_params(model_name, '.')\n",
        "  model_params.append({k: params[k] for k in model_runner.params.keys()})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koV-2_a6wbF5"
      },
      "source": [
        "###############\n",
        "# USER INPUT\n",
        "###############\n",
        "# native structure you want to pull active site from\n",
        "pos_idx_ref = [13,37,98] # note: zero indexed\n",
        "PDB_REF = \"afDesign/1QJG.pdb\"\n",
        "\n",
        "# starting structure (for random starting sequence, set PDB=None and LEN to desired length)\n",
        "pos_idx = [44,9,78]\n",
        "MODE = \"1.05_44_9_78_s524_r3_cce_adam\"\n",
        "PDB = f\"{MODE}.pdb\"\n",
        "LEN = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYiNDSxNwmVw"
      },
      "source": [
        "# prep reference (native) features\n",
        "OBJ_REF = protein.from_pdb_string(pdb_to_string(PDB_REF), chain_id=\"A\")\n",
        "SEQ_REF = jax.nn.one_hot(OBJ_REF.aatype,20)\n",
        "START_SEQ_REF = \"\".join([order_restype[a] for a in OBJ_REF.aatype])\n",
        "\n",
        "batch_ref = {'aatype': OBJ_REF.aatype,\n",
        "             'all_atom_positions': OBJ_REF.atom_positions,\n",
        "             'all_atom_mask': OBJ_REF.atom_mask}\n",
        "batch_ref.update(all_atom.atom37_to_frames(**batch_ref))\n",
        "batch_ref.update(prep_inputs.make_atom14_positions(batch_ref))\n",
        "\n",
        "# prep starting (design) features\n",
        "if PDB is not None:\n",
        "  OBJ = protein.from_pdb_string(pdb_to_string(PDB), chain_id=\"A\")\n",
        "  SEQ = jax.nn.one_hot(OBJ.aatype,20)\n",
        "  START_SEQ = \"\".join([order_restype[a] for a in OBJ.aatype])\n",
        "\n",
        "  batch = {'aatype': OBJ.aatype,\n",
        "          'all_atom_positions': OBJ.atom_positions,\n",
        "          'all_atom_mask': OBJ.atom_mask}\n",
        "  batch.update(all_atom.atom37_to_frames(**batch))\n",
        "  batch.update(prep_inputs.make_atom14_positions(batch))\n",
        "else:\n",
        "  SEQ = jnp.zeros(LEN).at[jnp.asarray(pos_idx)].set([OBJ_REF.aatype[i] for i in pos_idx_ref])\n",
        "  START_SEQ = \"\".join([order_restype[a] for a in SEQ])\n",
        "  SEQ = jax.nn.one_hot(SEQ,20)\n",
        "\n",
        "# prep input features\n",
        "feature_dict = {\n",
        "    **pipeline.make_sequence_features(sequence=START_SEQ,description=\"none\",num_res=len(START_SEQ)),\n",
        "    **pipeline.make_msa_features(msas=[N*[START_SEQ]], deletion_matrices=[N*[[0]*len(START_SEQ)]]),\n",
        "}\n",
        "inputs = model_runner.process_features(feature_dict, random_seed=0)\n",
        "\n",
        "if N > 1:\n",
        "  inputs[\"msa_row_mask\"] = jnp.ones_like(inputs[\"msa_row_mask\"])\n",
        "  inputs[\"msa_mask\"] = jnp.ones_like(inputs[\"msa_mask\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADmZt232wr8O",
        "outputId": "a9e3fed4-2a41-40cc-cc0b-2bae0b0f6f44"
      },
      "source": [
        "print([START_SEQ[i] for i in pos_idx])\n",
        "print([START_SEQ_REF[i] for i in pos_idx_ref])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Y', 'N', 'D']\n",
            "['Y', 'N', 'D']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wybq5_sIwt5S"
      },
      "source": [
        "########################################\n",
        "# losses to constrain backbone to starting design\n",
        "########################################\n",
        "def get_dgram_loss_(batch, outputs):\n",
        "  pb, pb_mask = model.modules.pseudo_beta_fn(batch[\"aatype\"],\n",
        "                                             batch[\"all_atom_positions\"],\n",
        "                                             batch[\"all_atom_mask\"])\n",
        "  \n",
        "  dgram_loss = model.modules._distogram_log_loss(outputs[\"distogram\"][\"logits\"],\n",
        "                                                 outputs[\"distogram\"][\"bin_edges\"],\n",
        "                                                 batch={\"pseudo_beta\":pb,\"pseudo_beta_mask\":pb_mask},\n",
        "                                                 num_bins=model_config.model.heads.distogram.num_bins)\n",
        "  return dgram_loss[\"loss\"]\n",
        "\n",
        "def get_fape_loss_(batch, outputs, use_clamped_fape=False):\n",
        "\n",
        "  sub_batch = jax.tree_map(lambda x: x, batch)\n",
        "  sub_batch[\"use_clamped_fape\"] = use_clamped_fape\n",
        "  loss = {\"loss\":0.0}    \n",
        "  folding.backbone_loss(loss, sub_batch, outputs[\"structure_module\"], model_config.model.heads.structure_module)\n",
        "  return loss[\"loss\"]\n",
        "\n",
        "#########################################\n",
        "# losses to constrain sidechains to active site\n",
        "#########################################\n",
        "def get_dgram_loss(batch, outputs, pos_idx, pos_idx_ref=None):\n",
        "  if pos_idx_ref is None: pos_idx_ref = pos_idx\n",
        "  pb, pb_mask = model.modules.pseudo_beta_fn(batch[\"aatype\"][pos_idx_ref],\n",
        "                                             batch[\"all_atom_positions\"][pos_idx_ref],\n",
        "                                             batch[\"all_atom_mask\"][pos_idx_ref])\n",
        "  \n",
        "  dgram_loss = model.modules._distogram_log_loss(outputs[\"distogram\"][\"logits\"][:,pos_idx][pos_idx,:],\n",
        "                                                 outputs[\"distogram\"][\"bin_edges\"],\n",
        "                                                 batch={\"pseudo_beta\":pb,\"pseudo_beta_mask\":pb_mask},\n",
        "                                                 num_bins=model_config.model.heads.distogram.num_bins)\n",
        "  return dgram_loss[\"loss\"]\n",
        "\n",
        "def get_fape_loss(batch, outputs, pos_idx, pos_idx_ref=None, backbone=True, sidechain=True, use_clamped_fape=False):\n",
        "  if pos_idx_ref is None: pos_idx_ref = pos_idx\n",
        "\n",
        "  sub_batch = jax.tree_map(lambda x: x[pos_idx_ref,...], batch)\n",
        "  sub_batch[\"use_clamped_fape\"] = use_clamped_fape\n",
        "\n",
        "  value = jax.tree_map(lambda x: x, outputs[\"structure_module\"])\n",
        "  loss = {\"loss\":0.0}\n",
        "  \n",
        "  if sidechain:\n",
        "    value.update(folding.compute_renamed_ground_truth(sub_batch, value['final_atom14_positions'][pos_idx,...]))\n",
        "    value['sidechains']['frames'] = jax.tree_map(lambda x: x[:,pos_idx,:], value[\"sidechains\"][\"frames\"])\n",
        "    value['sidechains']['atom_pos'] = jax.tree_map(lambda x: x[:,pos_idx,:], value[\"sidechains\"][\"atom_pos\"])\n",
        "    loss.update(folding.sidechain_loss(sub_batch, value, model_config.model.heads.structure_module))\n",
        "  \n",
        "  if backbone:\n",
        "    value[\"traj\"] = value[\"traj\"][...,pos_idx,:]\n",
        "    folding.backbone_loss(loss, sub_batch, value, model_config.model.heads.structure_module)\n",
        "\n",
        "  return loss[\"loss\"]\n",
        "\n",
        "def get_sidechain_rmsd_fix(batch, outputs, pos_idx, pos_idx_ref=None, include_CA=False):\n",
        "\n",
        "  if pos_idx_ref is None: pos_idx_ref = pos_idx\n",
        "  bb_atoms_to_exclude = [\"N\",\"O\"] if include_CA else [\"N\",\"CA\",\"O\"]\n",
        "\n",
        "  def kabsch(P, Q):\n",
        "    V, S, W = jnp.linalg.svd(P.T @ Q, full_matrices=False)\n",
        "    flip = jax.nn.sigmoid(-10 * jnp.linalg.det(V) * jnp.linalg.det(W))\n",
        "    S = flip * S.at[-1].set(-S[-1]) + (1-flip) * S\n",
        "    V = flip * V.at[:,-1].set(-V[:,-1]) + (1-flip) * V\n",
        "    return V@W\n",
        "\n",
        "  true_aa_idx = batch[\"aatype\"][pos_idx_ref]\n",
        "  true_pos = all_atom.atom37_to_atom14(batch[\"all_atom_positions\"],batch)[pos_idx_ref,:,:]\n",
        "  pred_pos = outputs[\"structure_module\"][\"final_atom14_positions\"][pos_idx,:,:]\n",
        "\n",
        "  i,j,j_alt = [],[],[]\n",
        "  i_non,j_non = [],[]\n",
        "  for n,aa_idx in enumerate(true_aa_idx):\n",
        "    aa = idx_to_resname[aa_idx]\n",
        "    atoms = residue_constants.residue_atoms[aa].copy()\n",
        "    for atom in atoms:\n",
        "      if atom not in bb_atoms_to_exclude:\n",
        "        i.append(n)\n",
        "        j.append(residue_constants.restype_name_to_atom14_names[aa].index(atom))\n",
        "        if aa in residue_constants.residue_atom_renaming_swaps:\n",
        "          swaps = residue_constants.residue_atom_renaming_swaps[aa]\n",
        "          swaps_rev = {v:k for k,v in swaps.items()}\n",
        "          if atom in swaps:\n",
        "            j_alt.append(residue_constants.restype_name_to_atom14_names[aa].index(swaps[atom]))\n",
        "          elif atom in swaps_rev:\n",
        "            j_alt.append(residue_constants.restype_name_to_atom14_names[aa].index(swaps_rev[atom]))\n",
        "          else:\n",
        "            j_alt.append(j[-1])\n",
        "            i_non.append(i[-1])\n",
        "            j_non.append(j[-1])\n",
        "        else:\n",
        "          j_alt.append(j[-1])\n",
        "          i_non.append(i[-1])\n",
        "          j_non.append(j[-1])\n",
        "\n",
        "  # align non-ambigious atoms\n",
        "  true_pos_non = true_pos[i_non,j_non,:]  \n",
        "  pred_pos_non = pred_pos[i_non,j_non,:]\n",
        "  true_pos = (true_pos - true_pos_non.mean(0)) @ kabsch(true_pos_non - true_pos_non.mean(0), pred_pos_non - pred_pos_non.mean(0))\n",
        "  pred_pos = pred_pos - pred_pos_non.mean(0)\n",
        "\n",
        "  true_pos_a = true_pos[i,j,:]\n",
        "  pred_pos_a = pred_pos[i,j,:]\n",
        "  pred_pos_b = pred_pos[i,j_alt,:]\n",
        "\n",
        "  rms_a = jnp.square(true_pos_a - pred_pos_a).sum(-1)\n",
        "  rms_b = jnp.square(true_pos_a - pred_pos_b).sum(-1)\n",
        "\n",
        "  return jnp.sqrt(jnp.minimum(rms_a,rms_b).mean() + 1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfdnXo9ywwVg"
      },
      "source": [
        "def get_grad_fn(model_runner, inputs, pos_idx_ref, inc_backbone=False):\n",
        "  \n",
        "  def mod(params, key, model_params, opt):\n",
        "    pos_idx = opt[\"pos_idx\"]\n",
        "    ############################\n",
        "    # set amino acid sequence\n",
        "    ############################\n",
        "    seq_logits = jax.random.permutation(key, params[\"msa\"])\n",
        "    seq_soft = jax.nn.softmax(seq_logits)\n",
        "    seq = jax.lax.stop_gradient(jax.nn.one_hot(seq_soft.argmax(-1),20) - seq_soft) + seq_soft\n",
        "    seq = seq.at[:,pos_idx,:].set(SEQ_REF[pos_idx_ref,:])\n",
        "\n",
        "    oh_mask = opt[\"oh_mask\"][:,None]\n",
        "    pseudo_seq = oh_mask * seq + (1-oh_mask) * seq_logits\n",
        "\n",
        "    inputs_mod = inputs.copy()\n",
        "    update_seq(pseudo_seq, inputs_mod, msa_input=(\"msa\" in params))\n",
        "\n",
        "    if \"msa_mask\" in opt:\n",
        "      inputs_mod[\"msa_mask\"] = inputs_mod[\"msa_mask\"] * opt[\"msa_mask\"][None,:,None]\n",
        "      inputs_mod[\"msa_row_mask\"] = inputs_mod[\"msa_row_mask\"] * opt[\"msa_mask\"][None,:]\n",
        "    \n",
        "    ####################\n",
        "    # set sidechains identity\n",
        "    ####################\n",
        "    B,L = inputs_mod[\"aatype\"].shape[:2]\n",
        "    ALA = jax.nn.one_hot(residue_constants.restype_order[\"A\"],21)\n",
        "\n",
        "    if \"msa\" in params:\n",
        "      aatype = jnp.zeros((B,L,21)).at[...,:20].set(seq[0])\n",
        "    else:\n",
        "      aatype = jnp.zeros((B,L,21)).at[...,:20].set(seq)\n",
        "\n",
        "    ala_mask = opt[\"ala_mask\"][:,None]\n",
        "    aatype_ala = jnp.zeros((B,L,21)).at[:].set(ALA)\n",
        "    aatype_ala = aatype_ala.at[:,pos_idx,:20].set(SEQ_REF[pos_idx_ref,:])\n",
        "    aatype_pseudo = ala_mask * aatype + (1-ala_mask) * aatype_ala\n",
        "    update_aatype(aatype_pseudo, inputs_mod)\n",
        "    \n",
        "    # get output\n",
        "    outputs = model_runner.apply(model_params, key, inputs_mod)\n",
        "\n",
        "    ###################\n",
        "    # structure loss\n",
        "    ###################\n",
        "    fape_loss = get_fape_loss(batch_ref, outputs, pos_idx, pos_idx_ref, backbone=inc_backbone, sidechain=True)\n",
        "    rmsd_loss = get_sidechain_rmsd_fix(batch_ref, outputs, pos_idx, pos_idx_ref)\n",
        "    dgram_loss = get_dgram_loss(batch_ref, outputs, pos_idx, pos_idx_ref)\n",
        "\n",
        "    losses = {\"fape\":fape_loss,\n",
        "              \"rmsd\":rmsd_loss,\n",
        "              \"dgram\":dgram_loss}\n",
        "\n",
        "    if \"sc_weight_fape\" in opt: fape_loss *= opt[\"sc_weight_fape\"]\n",
        "    if \"sc_weight_rmsd\" in opt: rmsd_loss *= opt[\"sc_weight_rmsd\"]\n",
        "    if \"sc_weight_dgram\" in opt: dgram_loss *= opt[\"sc_weight_dgram\"]\n",
        "\n",
        "    loss = (rmsd_loss + fape_loss + dgram_loss) * opt[\"sc_weight\"]\n",
        "  \n",
        "    ################### \n",
        "    # background loss\n",
        "    ###################\n",
        "    if \"conf_weight\" in opt:\n",
        "      pae = jax.nn.softmax(outputs[\"predicted_aligned_error\"][\"logits\"])\n",
        "      pae_loss = (pae * jnp.arange(pae.shape[-1])).sum(-1).mean()\n",
        "      plddt = jax.nn.softmax(outputs['predicted_lddt']['logits'])\n",
        "      plddt_loss = (plddt * jnp.arange(plddt.shape[-1])[::-1]).sum(-1).mean()\n",
        "\n",
        "      loss = loss + (pae_loss + plddt_loss) * opt[\"conf_weight\"]\n",
        "      losses[\"pae\"] = pae_loss\n",
        "      losses[\"plddt\"] = plddt_loss\n",
        "\n",
        "    if \"rg_weight\" in opt:\n",
        "      ca_coords = outputs[\"structure_module\"][\"final_atom_positions\"][:,1,:]\n",
        "      rg_loss = jnp.sqrt(jnp.square(ca_coords - ca_coords.mean(0)).sum(-1).mean() + 1e-8)\n",
        "      loss = loss + rg_loss * opt[\"rg_weight\"]\n",
        "      losses[\"rg\"] = rg_loss\n",
        "      \n",
        "    if \"bb_weight\" in opt:\n",
        "      fape_start_loss = get_fape_loss_(batch, outputs)      \n",
        "      dgram_start_loss = get_dgram_loss_(batch, outputs)\n",
        "      loss = loss + (dgram_start_loss + fape_start_loss) * opt[\"bb_weight\"]\n",
        "      losses[\"dgram_start\"] = dgram_start_loss\n",
        "      losses[\"fape_start\"] = fape_start_loss\n",
        "    \n",
        "    if \"msa\" in params and \"ent_weight\" in opt:\n",
        "      seq_prf = seq.mean(0)\n",
        "      ent_loss = -(seq_prf * jnp.log(seq_prf + 1e-8)).sum(-1).mean()\n",
        "      loss = loss + ent_loss * opt[\"ent_weight\"]\n",
        "      losses[\"ent\"] = ent_loss\n",
        "    else:\n",
        "      ent_loss = 0\n",
        "\n",
        "    outs = {\"final_atom_positions\":outputs[\"structure_module\"][\"final_atom_positions\"],\n",
        "            \"final_atom_mask\":outputs[\"structure_module\"][\"final_atom_mask\"]}\n",
        "\n",
        "    seq_ = seq[0] if \"msa\" in params else seq\n",
        "\n",
        "    return loss, ({\"losses\":losses, \"outputs\":outs, \"seq\":seq_})\n",
        "  loss_fn = mod\n",
        "  grad_fn = jax.value_and_grad(mod, has_aux=True, argnums=0)\n",
        "  return loss_fn, grad_fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vavxyvYJwyPC"
      },
      "source": [
        "# gradient function (note for greedy search we won't be using grad_fn, only loss_fn)\n",
        "loss_fn, grad_fn = get_grad_fn(model_runner, inputs, pos_idx_ref=pos_idx_ref)\n",
        "loss_fn = jax.jit(loss_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsLk1lLCQNcw"
      },
      "source": [
        "RMSD_min = np.inf\n",
        "key = jax.random.PRNGKey(0)\n",
        "L,A = len(START_SEQ),20\n",
        "\n",
        "pos_idx_ = jnp.asarray(pos_idx)\n",
        "pos_idx_ref_ = jnp.asarray(pos_idx_ref)\n",
        "\n",
        "msa = SEQ[None]\n",
        "#msa = jnp.zeros_like(SEQ[None]).at[:,pos_idx_].set(SEQ_REF[pos_idx_ref_])\n",
        "#msa = jax.nn.one_hot(jax.random.randint(key, (N,L), 0, A),A).at[:,pos_idx_].set(SEQ_REF[pos_idx_ref_])\n",
        "params = {\"msa\":msa}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFZY-ANvQgcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89ef657e-316f-4f7a-8099-f23e9882a97a"
      },
      "source": [
        "oh_mask = jnp.ones((L,))\n",
        "ala_mask = jnp.ones((L,))\n",
        "msa_mask = jnp.ones((N,))\n",
        "opt={\"oh_mask\":oh_mask,\n",
        "    \"msa_mask\":msa_mask,\n",
        "    \"ala_mask\":ala_mask,\n",
        "    #\"bb_weight\":0.1, \n",
        "    \"sc_weight\":1.0,\n",
        "    \"sc_weight_rmsd\":1.0,\n",
        "    \"sc_weight_fape\":1.0,\n",
        "    \"sc_weight_dgram\":0.0,\n",
        "    \"ent_weight\":0.0,\n",
        "    \"rg_weight\":0.0,\n",
        "    \"conf_weight\":0.01,\n",
        "    \"pos_idx\":pos_idx_,\n",
        "    }\n",
        "loss, outs = loss_fn(params, key, model_params[0], opt=opt)\n",
        "print(loss,outs[\"losses\"][\"rmsd\"],outs[\"losses\"][\"fape\"])\n",
        "save_pdb(outs,f\"{MODE}_starting.pdb\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.966159 1.0131341 0.43879992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7ZL2zc4t9I0"
      },
      "source": [
        "def mut(params):\n",
        "  while True:\n",
        "    i = np.random.randint(L)\n",
        "    a = np.random.randint(A)\n",
        "    if i not in pos_idx and params[\"msa\"][0,i,a] == 0:\n",
        "      params_ = params.copy()\n",
        "      params_[\"msa\"] = params[\"msa\"].at[:,i,:].set(jnp.eye(A)[a])\n",
        "      break\n",
        "  return params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwWFO1tvq3Gx"
      },
      "source": [
        "LOSS = loss\n",
        "OVERALL_RMSD = outs[\"losses\"][\"rmsd\"]\n",
        "OVERALL_FAPE = outs[\"losses\"][\"fape\"]\n",
        "OVERALL_LOSS = LOSS\n",
        "key = jax.random.PRNGKey(0)\n",
        "for n in range(1000):\n",
        "  buff_p,buff_l,buff_o = [],[],[]\n",
        "  for _ in range(20):\n",
        "    key,subkey = jax.random.split(key)\n",
        "    p = mut(params)\n",
        "    l,o = loss_fn(p, subkey, model_params[0], opt=opt)\n",
        "    buff_p.append(p); buff_l.append(l); buff_o.append(o)\n",
        "    if l < LOSS: break\n",
        "  best = jnp.argmin(jnp.asarray(buff_l))\n",
        "  params, LOSS, outs = buff_p[best], buff_l[best], buff_o[best]\n",
        "  RMSD = outs[\"losses\"][\"rmsd\"]\n",
        "  FAPE = outs[\"losses\"][\"fape\"]\n",
        "  if RMSD < OVERALL_RMSD:\n",
        "    OVERALL_RMSD = RMSD\n",
        "    save_pdb(outs,f\"{MODE}_best_rmsd.pdb\")\n",
        "  if FAPE < OVERALL_FAPE:\n",
        "    OVERALL_FAPE = FAPE\n",
        "    save_pdb(outs,f\"{MODE}_best_fape.pdb\")\n",
        "  if LOSS < OVERALL_LOSS:\n",
        "    OVERALL_LOSS = LOSS\n",
        "    save_pdb(outs,f\"{MODE}_best_loss.pdb\")\n",
        "  print(n, LOSS, RMSD, FAPE, len(buff_l))\n",
        "  n += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}